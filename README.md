[//]: # (>ðŸ“‹  A template README.md for code accompanying a Machine Learning paper)

# CamoDiffusion

This repository is the official implementation of [CamoDiffusion: Camouflaged Object Detection via
Conditional Diffusion Models](). 

Our implementation is based on the denoising diffusion repository from <a href="https://github.com/lucidrains/denoising-diffusion-pytorch">lucidrains</a>, which is a PyTorch implementation of <a href="https://arxiv.org/abs/2006.11239">DDPM</a>.

And we provide our pretrained weight and inference result in release.

[//]: # (>ðŸ“‹  Optional: include a graphic explaining your approach/main result, bibtex entry, link to demos, blog posts and tutorials)

## Requirements
- python == 3.9
- cuda == 11.3

To install requirements:

```setup
pip install -r requirements.txt
```

[//]: # (>ðŸ“‹  Describe how to set up the environment, e.g. pip/conda/docker commands, download datasets, etc...)

## Dataset
[COD (Camouflaged Object Detection) Dataset](https://github.com/lartpang/awesome-segmentation-saliency-dataset#camouflaged-object-detection-cod)

## Training

To train the model(s) in the paper, run this command:

```shell
accelerate launch train.py --config config/camoDiffusion_352x352.yaml --num_epoch=150 --batch_size=32 --gradient_accumulate_every=1
```
And then finetune it to 384 size:
```shell
accelerate launch train.py --config config/camoDiffusion_384x384.yaml --num_epoch=20 --batch_size=28 --gradient_accumulate_every=1 --pretrained model_352/model-best.pt --lr_min=0 --set optimizer.params.lr=1e-5
```

[//]: # (>ðŸ“‹  Describe how to train the models, with example commands on how to train the models in your paper, including the full training procedure and appropriate hyperparameters.)

## Evaluation
To test a model, run [sample.py](sample.py) with the desired model on different datasets:
```shell
accelerate launch sample.py \
  --config config/camoDiffusion_384x384.yaml \
  --results_folder ${RESULT_SAVE_PATH} \
  --checkpoint ${CHECKPOINT_PATH} \
  --num_sample_steps 10 \
  --target_dataset CAMO \
  --time_ensemble
```

For ease of use, we create a [eval.sh](scripts%2Feval.sh) script and a use case in the form of a shell script eval.sh.
You can edit the script to change the parameters you want to test.

```shell
bash scripts/eval.sh
```

[//]: # (>ðŸ“‹  Describe how to evaluate the trained models on benchmarks reported in the paper, give commands that produce the results &#40;section below&#41;.)

[//]: # (>ðŸ“‹  Include a table of results from your paper, and link back to the leaderboard for clarity and context. If your main result is a figure, include that figure and link to the command or notebook to reproduce it. )


## Tips

1. To implement the ATCN-Skip strategy, modify the config file by changing `SimpleDiffSeg` to `SimpleDiffSeg_Skip`. This adjustment allows the `ATCN` computation to be skipped every alternate time step, minimizing computational redundancy.

2. The pretrained model weights have been uploaded in the release.
The pretrained weights for SOD will be uploaded soon.
## Citation

[//]: # (>ðŸ“‹  Pick a licence and describe how to contribute to your code repository. )
```
@article{chen2023camodiffusion,
  title={CamoDiffusion: Camouflaged Object Detection via Conditional Diffusion Models},
  author={Chen, Zhongxi and Sun, Ke and Lin, Xianming and Ji, Rongrong},
  journal={arXiv preprint arXiv:2305.17932},
  year={2023}
}
```


```
@article{sun2025conditional,
  title={Conditional Diffusion Models for Camouflaged and Salient Object Detection},
  author={Sun, Ke and Chen, Zhongxi and Lin, Xianming and Sun, Xiaoshuai and Liu, Hong and Ji, Rongrong},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2025},
  publisher={IEEE}
}
```